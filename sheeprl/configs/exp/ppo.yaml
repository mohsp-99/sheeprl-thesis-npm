# @package _global_

defaults:
  - override /algo: ppo
  - override /env: maniskill
  - override /model_manager: ppo
  - _self_

# Algorithm
algo:
  total_steps: 1500000
  per_rank_batch_size: 32
  mlp_keys:
    encoder: [state]
  cnn_keys:
    encoder: []
  gamma: 0.8
  vf_coef: 0.5
  update_epochs: 4
  dense_units: 256
  mlp_layers: 3




# Distribution
distribution:
  type: "auto"

# Buffer
buffer:
  share_data: False
  size: ${algo.rollout_steps}

optimizer:
  lr: 3e-4
  eps: 1e-4

metric:
  aggregator:
    metrics:
      Loss/value_loss:
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/policy_loss:
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
      Loss/entropy_loss:
        _target_: torchmetrics.MeanMetric
        sync_on_compute: ${metric.sync_on_compute}
